{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb890d47",
   "metadata": {},
   "source": [
    "### Practical 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a56473",
   "metadata": {},
   "source": [
    "### Data Preprocessing on EmployeeRecords Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121d479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------Basic Machine learning principle: ---------------------------------\n",
    "#The features also called independent variables are the variables containing some information \n",
    "#using which we can predict the information contained in so called dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b14009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b296b329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---------------------------------Step 1:- Importing the dataset-----------------------------\n",
    "\n",
    "# importing dataset \"EmployeeRecords.csv\" and integrate to a variable \"dataset\"\n",
    "# This step reads all the values of this dataset and it will create a Pandas \"DataFrame\".\n",
    "dataset = pd.read_csv(\"D:/ML Dataset/EmployeeRecords.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e7413d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "#---------------------------Step 2 : Follow basic principle in Machine Learning------------------------------\n",
    "\n",
    "#In any dataset with which we are going to train a machinert model we have the same entities.\n",
    "# which are the features and the dependent variable vector.\n",
    "# Features or Independent variables are the columns used to predict the so called dependent variable.\n",
    "# Dependent variables is generally the last column of dataset.\n",
    "# This is because last column may give some valuable predictions based on previous columns.\n",
    "\n",
    "\n",
    "# ------------------------Step 3 : Retrieving Independent Features and Dependent variables in x and y-------------------\n",
    "\n",
    "#The iloc function will fetch all rows of all columns except last one\n",
    "# The first parameter in iloc is to fetch rows. Simply ':' without any indexes will fetch all rows.\n",
    "# The second parameter ':-1' denotes a range that will fetch all columns except last column.\n",
    "#x refers to independent variable\n",
    "\n",
    "X = dataset.iloc[:, : -1].values\n",
    "\n",
    "# -------------------------------Retrieving dependent variable i.e. last column ----------------------------------\n",
    "\n",
    "# The iloc function will fetch all rows of only last column.\n",
    "# The second parameter ':-1' will fetch only last column.\n",
    "# y refers to dependent variable\n",
    "\n",
    "y = dataset.iloc[:,-1].values\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94440622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------Taking care of missing data---------------------------------------\n",
    "\n",
    "# In real world datasets they may contain missing values, blanks, NaNs or other placeholders.\n",
    "# Such datasets however are incompatible  with scikit-learn estimators\n",
    "# which assume that all values in an array are numerical, and that all have and hold meaning.\n",
    "# one solution is to discard entire rows and/or columns of dataset containing missing values.\n",
    "# however it may lead to loss of valuable data\n",
    "# Better strategy is to impute the missing values, i.e., to infer them from the known part of data \n",
    "\n",
    "# For this we use SimpleImputer class from sklearn.impute module\n",
    "# the SimpleImputer class provides basic strategies for imputing missing values.\n",
    "# Missing values are imputed using constant value, or statistics(mean, median or 'most frequent value')\n",
    "# of each column in which the missing values are located.\n",
    "# This class also allows for different missing values encodings.\n",
    "\n",
    "# Here we replace missing values, encodes as np.nan, using the mean value of the column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f28bd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np\\nimport pandas as pd'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------Step 4 From sklearn library import sklearn.impute module--------------------------------\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ------------------------Step 5 Create an instance of the SimpleImputer class---------------------------------\n",
    "\n",
    "# The poarameters used in creating SimpleImputer object will enable replacing nulls with mean value.\n",
    "\n",
    "#1. First argument 'missing_values' specifies which value we have to replace.\n",
    "#2. Second argument 'strategy' specifies the replacement policy i.e. mean, median or most_frequent.\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc36dd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------Step 6: Use fit() to apply imputer object to matriz of features-----------------------\n",
    "\n",
    "# The object created in above step is not yet connected to the dataset matrix of features.\n",
    "# IIn this step we will apply this imputer object on the matrix of features.\n",
    "# For this we use fit method that will connect this imputer object to the matrix of features.\n",
    "\n",
    "# The fit method observes missing values in specified columns and computes average of values in the.\n",
    "\n",
    "#                                   Syntax : fit(X[rows,columns])\n",
    "\n",
    "# Fit the imputer on the rows in the columns in matrix of features 'X'.\n",
    "\n",
    "# Here 'X' denotes the matrix of features where we want to replace the missing data.\n",
    "# Inside 'X' first specify the range of rows that this fit method will read.\n",
    "# Then in 'X' specify the range of columns to look for some missing data\n",
    "# Note: We must specify colums with only numeric values or real numbers like age, salary etc.\n",
    "\n",
    "imputer.fit(X[:,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41fae480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------- Step 7: Call transform() method using imputer object -------------------------------\n",
    "# It will replace the missing values of each column with specified value i.e. mean value here.\n",
    "\n",
    "#                                   Syntax : transform(X)\n",
    "\n",
    "# => Impute all missing values in X and return transformed columns.\n",
    "# Transform method returns the new updated version of the matrix of features.\n",
    "# Thus, on left side the lvalue will be same as 'X' matrix of features on right side of transform().\n",
    "\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "886dbf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------Encoding Categorical data -------------------------------------\n",
    "\n",
    "# Encoding categorical data i.e. Encoding the Independent Variable\n",
    "\n",
    "# It may be difficult for a Machine Learning model to compute correlations between:\n",
    "#    Categorical Strings based features and outcome based dependent variable column.\n",
    "#    Thus, its necessary to convert or encode these strings i.e. categories into relevant numbers.\n",
    "#    For this we use OneHotEncoding\n",
    "\n",
    "# OneHot Encoding consists of turning the categorical column into 'n' no. of columns.\n",
    "# 'n' no. refers to the 'n' different classes wil be created, if 5 classes then 5 columns and so on.\n",
    "# Example : It it has 3 classes then 3 columns will be created, if it has 5 then 5 columns will be created.\n",
    "# OneHotEncoding consists of creating binary vectors for each of the class present.\n",
    "# Encoding with binary variate like 100, 010, 001 etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27e17732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------Step 8 : OneHotEncoding for encoding categorical data------------------------------\n",
    "\n",
    "# A. Importing ColumnTransformer class from sklearn.compose module.\n",
    "\n",
    "from sklearn. compose import ColumnTransformer\n",
    "\n",
    "# B. sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# C. The next step is to create an object of the ColumnTransformer a class.\n",
    "\n",
    "# It takes two arguments:\n",
    "\n",
    "# 1. First are Transformers that specify tuple of 3 things: )1. kind of transformation i.e. encoding,\n",
    "#   (2.) type of encoding i.e. OneHotEncoder and (3.) indexes of columns we want to encode.\n",
    "# 2. Second argument is remainder that specifies the columns that will be passed through and not encoded\n",
    "\n",
    "#         Syntax : ColumnTransformer(transformers=[(kind of transformation i.e. encoding, kind of encoding, \n",
    "#                                                                              column_index), remainder])\n",
    "\n",
    "# remainder - defines what to do with others\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(),[0])], remainder='passthrough')\n",
    "\n",
    "# D. Next step is to connect 'ct' object to matrix of features 'X' in a single step using fit_transform()\n",
    "# The fit_transform() will fit and transform 'X', at same time.\n",
    "# the fit_transform() will return the new matrix of features 'X' with 'n' columns OneHotEncoder\n",
    "# The train() used later in Machine Learning model will expect the matrix of features x as numpy array.\n",
    "# So we force and cast the output of this fit_transform() method to be a numpy array.\n",
    "\n",
    "X = np.array(ct.fit_transform(X))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d46237d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------- Step 9 : Label Encoding ------------------------------------------\n",
    "\n",
    "# Encoding the dependent Variable.\n",
    "# for this use another class called Label Encoder which encodes the labels in zeros and ones respectively.\n",
    "\n",
    "# A. Importing LabelEncoder class from sklearn.preprocessing module.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# B. Next we create an object of this LabelEncoder class.\n",
    "# No parameters required since it is applied on the one single vector needed to encode.\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# C. Use fit_transform() to fit and transform 'le' object to 'y' dependent variable and \n",
    "# convert text in numerical values directly.\n",
    "# Here no need to convert to numpy \n",
    "# LHS and RHS contain same 'y'.\n",
    "\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(y)\n",
    "\n",
    "# Note : We must apply OneHotEncoding when we have several categories in one of the features of your \n",
    "# matrix of features but also you can do a simple LabelEncoding when we have two classes\n",
    "# which we can directly encode into zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e26a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------Data Splitting and Feature Scaling-------------------------------------\n",
    "\n",
    "#Values should be within the range of -3 to +3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc557c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------Step 10 : Dataset Splitting -----------------------------\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "# Sklearn library contains a module called module_selection contains function called train_test_split()\n",
    "\n",
    "# train_test_split() function will create 4 different set:\n",
    "# i.e. a pair of matrix of features and dependent variable for training set and \n",
    "# a pair of matrix of features and dependent variable for test set.\n",
    "# The set X_train is a matrix of features of the training set.\n",
    "# The set X_test is the matrix of features of the test set.\n",
    "# The set y_train is a dependent variable of training set.\n",
    "# The set y_test is a dependent variable of the test set.\n",
    "\n",
    "# Most Machine Learning models expect this format of inputs.\n",
    "# For training it expects X_train & Y_train as inputs in fit() method.\n",
    "# For the predictions also called inference these models will predict X_test.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =2/10, random_state = 1)\n",
    "\n",
    "#test_size :- \"float or int value -0.0 - 1.0\" It represents the proportion of dataset to include in the test split.\n",
    "#             The value range is between 0.0 and 1.0 Default value is 0.25\n",
    "#             Here test size is set 0.2 i.e. 20% of total observations will be there in test set\n",
    "#             and 80% will go in training set.\n",
    "\n",
    "#train_size :- float or int. It represents the proportion of dataset to include in the train split.\n",
    "#              The value range is between 0.0 - 1.0. Default value is complement of the test size.\n",
    "\n",
    "\n",
    "# Random_state: - Since the observations will be randomly split into the training set and test set so we are just \n",
    "# fixiing the seed in random_state = 1 so that we get the same split and therefore the same training set and same test set.\n",
    "\n",
    "#              Note : It doesn't matter if the random_state is 0 or 1 or any other integer.\n",
    "#              What natters is that it snould be set the same value, if we want to validate\n",
    "#              our processing over multiple runs of code. Setting randon_state to a fixed value\n",
    "#              will guarantee that same sequence of random numbers are generated each time you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "263daf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f72ac203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f38e5f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55f3f283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7ad7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- Step 11 : Feature Scaling ---------------------------------\n",
    "\n",
    "# Feature Scaling\n",
    "# Feature Scaling is a technique to standardize the independent features present in the data\n",
    "# in a fixed range.\n",
    "# It is performed during the data pre-processing to handLe highly varying values.\n",
    "# If feature scaling is not done, then an ML algorithm tends to weigh greater values, higher\n",
    "# and consider smaller values as the lower values, regardless of the unit of the values.\n",
    "# Thus, greater values tend to dominate smatLer values.\n",
    "# Example: If an algorithm is not using feature scaling method then it may consider the value\n",
    "# 4000 meters to be greater than 6 km which is false. The algorithm here may give wrong predictions.\n",
    "\n",
    "# SO, we use Feature Scaling to bring all values to same magnitudes and thus, solve this issue.\n",
    "# Note : We won't have to apply feature scaling for all the Machine learning models at all t times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28f19aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example : \n",
    "# In our dataset the range of Age feature is 27.50, whereas range of Salary is 48000 - 83000\n",
    "# It shows that range of Salary is much wider than the range of Age.\n",
    "# The difference in Age contributes less compared to salary to the overall difference in feature set.\n",
    "# Thus, age feature will be dominated by salary feature if we do not apply feature scaling.\n",
    "# Therefore, we should use Feature Scaling to bring all values to the same magnitudes to solve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4b378d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two methods of feature Scaling\n",
    "\n",
    "# 1 .Standardisation and 2. Normalisation\n",
    " \n",
    "#    Xstand = X - mean(X)/ standard deviation (X)\n",
    "\n",
    "#    Xnorm = X-min(X)/ (max(X) - min(X))\n",
    "# if outliers are there use Standardisation otherwise use Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e3f4444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In contrast to Standardisation, we obtain smaller standard deviations through \"Max-Min Normalisation,\n",
    "# Applying \"Max-Min Nomaralisation generates smaller standard deviations than using Standardisation.\n",
    "# It implies that data is more concentrated around mean if we scale data using Max-Min Noraralisation.\n",
    "# Thus, if we have outliers in a feature then normalizing data scale data to a small intervol\n",
    "# standardization does not have a bounding range.\n",
    "# So, even if you have outliers in your data, they not be affected by standardization.\n",
    "# Standardisation is more robust to outliers and is preferable over Max-Min Normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3388e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4 : Do we need to apply feature scaling to the dummy variables i.e. columns[0,1,2]\n",
    "# in the matrix of features i.e. encoded values?\n",
    "\n",
    "# Answer : The answer is No. The goal of standardization or feature scaling is to have all the values\n",
    "# of the features in the same range. Standardization actually transforms features so that \n",
    "# they take values between more or less -3 and 3 approx. Since here are dummy variables already\n",
    "# take values between this range because they're equal to either 1 or 0.\n",
    "# So, nothing extra needs to be done here regarding standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd0456da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sklearn.preprocessing module that contains the StandardScaler class.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Now create an object of the StandardScaler class\n",
    "# Note :  There are no arguments to input since we just want  to get the mean and standard deviation and \n",
    "# then apply this formula to all the values in the feature.\n",
    "# This will be done automatically and no parameters are needed for this.\n",
    "\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "770fbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we fit our scalar Standardization toot on the training set X_train.\n",
    "# Based on above discussion we won't apply feature scaling on the dummy variables and\n",
    "# we will fit our standard scalar object only on ages and the salaries columns.\n",
    "# We take all the rows of these two data columns. Here in X_train[;, 3:] the first ':'refers to all\n",
    "# rows and '3:' refers to column range from age and salary column.\n",
    "# Remember to take only all those columns with numerical values from large matrix of features.\n",
    "\n",
    "# Applying the formulas:\n",
    "# 1. Fit() : It only compute the mean and the standard deviation of all values of age and salary.\n",
    "\n",
    "# 2. Transform() : Transform method that will indeed apply standardization formula by transforming\n",
    "#                  each of the values of each feature (Age and Salary) into the 'xstand' value\n",
    "#                  resulting from standardization formula.\n",
    "\n",
    "# Thus, the fit() just get the mean and standard deviation of each of your features and\n",
    "# transform() will apply the standardization formula to transform values for then to be in same scale.\n",
    "\n",
    "# 3. fit_transform : A method of the StandardScaLer class that will follow the two functions\n",
    "#                    at the some time meaning it will fit the matrtx of features to give the mean\n",
    "#                    and standard deviation and after tnat transform all the values of the features\n",
    "#                    as per standardization formula.\n",
    "\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c23138ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will transform our matrix of features of test set meaning X_test.\n",
    "# For this data we will only apply the transform method because indeed the features of the test set\n",
    "#need to be scaled by the same scalar that was used on the training set.\n",
    "#Here we will not use fit_transform()  on X_test because it would generate a new scalar\n",
    "#and that would absoloutely not make sense because X_test will actually\n",
    "# predict function that will produce predictions in later stages of ML models.\n",
    "\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10b3dd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
     ]
    }
   ],
   "source": [
    "# On printing X_train we will find that we get same values for the dummy variables which\n",
    "# are indeed still between minus three and plus three.\n",
    "# The age and salary variables were transformed so that they take new values between minus two and plus two.\n",
    "# Thus, they are now on same scale.\n",
    "# This will help to improve or optimize the training of certain ML models.\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15482fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "62d38aa2568000845c3a49e08968b7b7a3b043358443f49bc32e837941d4ae7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
